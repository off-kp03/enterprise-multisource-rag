import os
from abc import ABC, abstractmethod
from typing import Dict
from openai import OpenAI



class BaseLLMClient(ABC):
    """
    Abstract base class for all LLM clients.
    """

    @abstractmethod
    def generate(self, prompts: Dict[str, str]) -> str:
        """
        Takes system_prompt and user_prompt, returns generated text.
        """
        pass

class MockLLMClient(BaseLLMClient):
    """
    Zero-cost mock LLM for development and testing.
    Produces grounded answers with citations.
    """

    def generate(self, prompts: Dict[str, str]) -> str:
        user_prompt = prompts.get("user_prompt", "")

        # Very simple, deterministic "answer"
        answer_lines = [
            "Based on the provided sources, here is a grounded answer:\n",
            "The documents indicate the following key points:\n",
        ]

        # Extract source numbers from the prompt
        sources_used = []
        for line in user_prompt.splitlines():
            if line.startswith("[Source"):
                sources_used.append(line.strip())

        if sources_used:
            answer_lines.append("\nSources used:\n")
            for src in sources_used:
                answer_lines.append(src)

        answer_lines.append(
            "\n(Note: This answer was generated by a mock LLM for testing.)"
        )

        return "\n".join(answer_lines)


class OpenAILikeClient(BaseLLMClient):
    """
    OpenAI / Azure OpenAI compatible client.
    Actual API call will be implemented later.
    """

    def __init__(self, model_name: str):
        self.model_name = model_name
        self.api_key = os.getenv("LLM_API_KEY")

        if not self.api_key:
            raise RuntimeError(
                "LLM_API_KEY environment variable is not set"
            )

    def generate(self, prompts: Dict[str, str]) -> str:
        client = OpenAI(api_key=self.api_key)

        response = client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": prompts["system_prompt"]},
                {"role": "user", "content": prompts["user_prompt"]},
            ],
            temperature=0.2,
        )

        return response.choices[0].message.content
